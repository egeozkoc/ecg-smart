{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bLkllEOtFY5",
        "outputId": "10071d5a-2c98-4d9e-c7a2-3e7ecf54c1e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fairlearn in /opt/anaconda3/lib/python3.12/site-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /opt/anaconda3/lib/python3.12/site-packages (from fairlearn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=2.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from fairlearn) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from fairlearn) (1.4.2)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /opt/anaconda3/lib/python3.12/site-packages (from fairlearn) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=2.0.3->fairlearn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=2.0.3->fairlearn) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=2.0.3->fairlearn) (2023.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=1.2.1->fairlearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=1.2.1->fairlearn) (2.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->fairlearn) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install fairlearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "0uYFI-j0UvSJ"
      },
      "outputs": [],
      "source": [
        "#Libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, roc_auc_score, confusion_matrix, accuracy_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ranksums\n",
        "import statistics\n",
        "import math\n",
        "from scipy.interpolate import interp1d\n",
        "from numpy.random import seed\n",
        "seed(42)\n",
        "from tensorflow import random\n",
        "random.set_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tFhvbOHuU2w7"
      },
      "outputs": [],
      "source": [
        "#Variables\n",
        "threshold = 0.15\n",
        "num_samples = 100\n",
        "specified_specificity = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "uUmvF7XMU43y"
      },
      "outputs": [],
      "source": [
        "#Basic Functions\n",
        "def preprocess_data(df_train, df_test): #could be df_train and df_val during CV\n",
        "    #Standardize the data and impute nan data- based on training, create pipeline\n",
        "    if not df_train.select_dtypes(include=['category', 'int']).empty:\n",
        "        print('Warning: There is categorical data in your training data-set')\n",
        "    num_cols = df_train.select_dtypes(include=['float']).columns\n",
        "    pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('normalizer', Normalizer())\n",
        "        ])\n",
        "    df_train = pipeline.fit_transform(df_train)\n",
        "    df_test = pipeline.transform(df_test)\n",
        "    return(df_train, df_test)\n",
        "\n",
        "\n",
        "\n",
        "def train(df_train, dfy_train):\n",
        "    clf = RandomForestClassifier(criterion='gini', bootstrap=(True), class_weight='balanced_subsample', random_state=42)\n",
        "    clf_trained = clf.fit(df_train, dfy_train)\n",
        "    return(clf_trained)\n",
        "\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "def test(clf_trained, df_test, dfy_test, threshold, test=False):\n",
        "    proba_test = clf_trained.predict(df_test, verbose=0)\n",
        "    y_pred_test = proba_test[:,1] > threshold\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(dfy_test[:,1], y_pred_test).ravel()\n",
        "    sen = tp / (tp + fn)\n",
        "    spe = tn / (tn + fp)\n",
        "    FNR = fn / (tp + fn)\n",
        "    if test==True:\n",
        "        return(sen, spe, FNR, y_pred_test, proba_test[:,1])\n",
        "    return(sen, spe, FNR)\n",
        "\n",
        "\n",
        "def evaluate_disparities(clf, proba_test, X_test, y_test, test_ind, ind_test_b, ind_test_nb):\n",
        "  np.random.seed(0)\n",
        "  y_test_df = pd.DataFrame(data=y_test[:,1], index=test_ind)\n",
        "  #Get bootstrapped sen, spe, FNR test array for df_b:\n",
        "  proba_test_b = pd.DataFrame(data=proba_test, index=test_ind).loc[ind_test_b,:].values\n",
        "  df_test_b = pd.DataFrame(data=X_test, columns=cols, index=test_ind).loc[ind_test_b,:].values\n",
        "  dfy_test_b = y_test_df.loc[ind_test_b].values\n",
        "  df_sen_b_b = []\n",
        "  df_spe_b_b = []\n",
        "  df_tpr_b_b = []\n",
        "  df_auc_b_b = []\n",
        "  for i in np.arange(num_samples):\n",
        "    idx = np.random.randint(0, df_test_b.shape[0], (df_test_b.shape[0],))\n",
        "    #sen_bootstrap_b, spe_bootstrap_b, FNR_bootstrap_b = test(clf, df_test_b[idx,:], dfy_test_b[idx], threshold)\n",
        "    fpr_bootstrap_b, tpr_bootstrap_b, _ = roc_curve(dfy_test_b[idx], proba_test_b[idx])\n",
        "    tpr_at_target_fpr = np.interp(0.2, fpr_bootstrap_b, tpr_bootstrap_b)\n",
        "    auc_bootstrap_b = roc_auc_score(dfy_test_b[idx], proba_test_b[idx])\n",
        "    #df_sen_b_b.append(sen_bootstrap_b)\n",
        "    #df_spe_b_b.append(spe_bootstrap_b)\n",
        "    df_tpr_b_b.append(tpr_at_target_fpr)\n",
        "    df_auc_b_b.append(auc_bootstrap_b)\n",
        "  sorted = np.sort(df_tpr_b_b)\n",
        "  tpr_lower_bound_b = np.percentile(sorted, (1 - 0.95) / 2 * 100)\n",
        "  tpr_upper_bound_b = np.percentile(sorted, (1 + 0.95) / 2 * 100)\n",
        "  sorted = np.sort(df_auc_b_b)\n",
        "  auc_lower_bound_b = np.percentile(sorted, (1 - 0.95) / 2 * 100)\n",
        "  auc_upper_bound_b = np.percentile(sorted, (1 + 0.95) / 2 * 100)\n",
        "\n",
        "  #Get bootstrapped sen, spe, FNR test array for df_nb:\n",
        "  proba_test_nb = pd.DataFrame(data=proba_test, index=test_ind).loc[ind_test_nb,:].values\n",
        "  df_test_nb = pd.DataFrame(data=X_test, columns=cols, index=test_ind).loc[ind_test_nb,:].values\n",
        "  dfy_test_nb = y_test_df.loc[ind_test_nb].values\n",
        "  df_sen_b_nb = []\n",
        "  df_spe_b_nb = []\n",
        "  df_tpr_b_nb = []\n",
        "  df_auc_b_nb = []\n",
        "  for i in np.arange(num_samples):\n",
        "    idx = np.random.randint(0, df_test_nb.shape[0], (df_test_nb.shape[0],))\n",
        "    #sen_bootstrap_nb, spe_bootstrap_nb, FNR_bootstrap_nb = test(clf, df_test_nb[idx,:], dfy_test_nb[idx], threshold)\n",
        "    fpr_bootstrap_nb, tpr_bootstrap_nb, _ = roc_curve(dfy_test_nb[idx], proba_test_nb[idx])\n",
        "    tpr_at_target_fpr = np.interp(0.2, fpr_bootstrap_nb, tpr_bootstrap_nb)\n",
        "    auc_bootstrap_nb = roc_auc_score(dfy_test_nb[idx], proba_test_nb[idx])\n",
        "    #df_sen_b_nb.append(sen_bootstrap_nb)\n",
        "    #df_spe_b_nb.append(spe_bootstrap_nb)\n",
        "    df_tpr_b_nb.append(tpr_at_target_fpr)\n",
        "    df_auc_b_nb.append(auc_bootstrap_nb)\n",
        "\n",
        "  sorted = np.sort(df_tpr_b_nb)\n",
        "  tpr_lower_bound_nb = np.percentile(sorted, (1 - 0.95) / 2 * 100)\n",
        "  tpr_upper_bound_nb = np.percentile(sorted, (1 + 0.95) / 2 * 100)\n",
        "  sorted = np.sort(df_auc_b_nb)\n",
        "  auc_lower_bound_nb = np.percentile(sorted, (1 - 0.95) / 2 * 100)\n",
        "  auc_upper_bound_nb = np.percentile(sorted, (1 + 0.95) / 2 * 100)\n",
        "\n",
        "  #Comparison df no sex female vs df no sex male: test\n",
        "  #_, pvalue11 = ranksums(x=df_sen_b_b, y=df_sen_b_nb)\n",
        "  #_, pvalue12 = ranksums(x=df_spe_b_b, y=df_spe_b_nb)\n",
        "  _, pvalue13 = ranksums(x=df_auc_b_b, y=df_auc_b_nb)\n",
        "  print('Comparison df black vs df non-black for AUC:', format(pvalue13,\"e\"))\n",
        "\n",
        "  #Final ROC plotting\n",
        "  proba_test_df = pd.DataFrame(data=proba_test, index=test_ind)\n",
        "  y_test = pd.DataFrame(data=y_test[:,1], index = test_ind)\n",
        "\n",
        "  #ROC Black\n",
        "  fpr_b, tpr_b, _ = roc_curve(y_test.loc[ind_test_b], proba_test_df.loc[ind_test_b])\n",
        "  roc_auc_score_b = roc_auc_score(y_test.loc[ind_test_b], proba_test_df.loc[ind_test_b])\n",
        "  interp_func = interp1d(fpr_b, tpr_b)\n",
        "  specified_sensitivity = interp_func(1 - specified_specificity)\n",
        "\n",
        "\n",
        "\n",
        "  #ROC Non-Black\n",
        "  fpr_nb, tpr_nb, _ = roc_curve(y_test.loc[ind_test_nb], proba_test_df.loc[ind_test_nb])\n",
        "  roc_auc_score_nb = roc_auc_score(y_test.loc[ind_test_nb], proba_test_df.loc[ind_test_nb])\n",
        "  interp_func = interp1d(fpr_nb, tpr_nb)\n",
        "  specified_sensitivity = interp_func(1 - specified_specificity)\n",
        "  return pvalue13, auc_lower_bound_b, auc_upper_bound_b, auc_lower_bound_nb, auc_upper_bound_nb, tpr_lower_bound_b, tpr_upper_bound_b, tpr_lower_bound_nb, tpr_upper_bound_nb\n",
        "\n",
        "\n",
        "def difference(proba_test, X_test, y_test, test_ind, ind_test_b, ind_test_nb):\n",
        "  y_test = pd.DataFrame(data=y_test[:,1], index = test_ind)\n",
        "  proba_test_df = pd.DataFrame(data=proba_test, index=test_ind)\n",
        "  #ROC Black\n",
        "  fpr_b, tpr_b, _ = roc_curve(y_test.loc[ind_test_b], proba_test_df.loc[ind_test_b])\n",
        "  roc_auc_score_b = roc_auc_score(y_test.loc[ind_test_b], proba_test_df.loc[ind_test_b])\n",
        "  interp_func = interp1d(fpr_b, tpr_b)\n",
        "  specified_sensitivity_b = interp_func(1 - specified_specificity)\n",
        "\n",
        "\n",
        "  #ROC Non-Black\n",
        "  fpr_nb, tpr_nb, _ = roc_curve(y_test.loc[ind_test_nb], proba_test_df.loc[ind_test_nb])\n",
        "  roc_auc_score_nb = roc_auc_score(y_test.loc[ind_test_nb], proba_test_df.loc[ind_test_nb])\n",
        "  interp_func = interp1d(fpr_nb, tpr_nb)\n",
        "  specified_sensitivity_nb = interp_func(1 - specified_specificity)\n",
        "\n",
        "  return specified_sensitivity_nb - specified_sensitivity_b\n",
        "\n",
        "class CustomLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self, predictor, adversary, y_race, X_ad, a, b):\n",
        "      super().__init__()\n",
        "      self.predictor = predictor\n",
        "      self.adversary = adversary\n",
        "      self.y_race = y_race\n",
        "      self.X_ad = X_ad\n",
        "      self.a = a\n",
        "      self.b = b\n",
        "\n",
        "  def call(self, y_true, y_pred):\n",
        "      predictor_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "\n",
        "      batch_size = tf.shape(y_pred)[0]\n",
        "      idx = tf.random.uniform([batch_size], minval=0, maxval=tf.shape(self.X_ad)[0], dtype=tf.int32)\n",
        "      X_ad = tf.gather(self.X_ad, idx)\n",
        "\n",
        "      y_race = tf.gather(self.y_race, idx)\n",
        "\n",
        "      adversary_pred = self.adversary(X_ad)\n",
        "      adversary_loss = tf.keras.losses.binary_crossentropy(y_race, adversary_pred)\n",
        "\n",
        "\n",
        "      combined_loss = predictor_loss - self.b*predictor_loss/adversary_loss - self.a*adversary_loss\n",
        "\n",
        "      return combined_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "vmLBDpJgVG61",
        "outputId": "ec6f1abe-c895-48cc-b787-84ae4926a4f1"
      },
      "outputs": [],
      "source": [
        "from joblib import load\n",
        "\n",
        "X_test = \"External validation features\"\n",
        "y_test = \"External validation labels (ACS = 1, no ACS = 0)\"\n",
        "y_test = y_test.astype(int)\n",
        "ID_test = X_test.index\n",
        "\n",
        "#Race indices\n",
        "race_test = X_test['race']\n",
        "race_test = race_test.astype(int)\n",
        "ind_test_nb = X_test[X_test['race']==0].index\n",
        "ind_test_b = X_test[X_test['race']==1].index\n",
        "\n",
        "\n",
        "#Drop sensitive features\n",
        "X_test.drop(['race'],axis = 1, inplace=True)\n",
        "\n",
        "df = pd.read_excel('feature_importance_final.xlsx')\n",
        "selected_features = df['feature'].tolist()\n",
        "X_test = X_test[selected_features[:148]] #select top 148 features\n",
        "cols = X_test.columns\n",
        "\n",
        "\n",
        "scaler1 = load('scaler1.joblib')\n",
        "X_test = scaler1.transform(X_test)\n",
        "X_test = pd.DataFrame(data=X_test, columns = cols, index=ID_test)\n",
        "\n",
        "\n",
        "#Top features selected by RF\n",
        "top_indices = np.load('rf_chosen_100features.npy', allow_pickle=True).tolist()\n",
        "X_test = X_test[top_indices]\n",
        "cols = X_test.columns\n",
        "\n",
        "scaler2 = load('scaler2.joblib')\n",
        "X_test = scaler2.transform(X_test)\n",
        "y_test = to_categorical(y_test, num_classes=2)\n",
        "\n",
        "\n",
        "model_p = tf.keras.models.load_model('model_p.h5', compile=False)\n",
        "\n",
        "\n",
        "print('Testing')\n",
        "sen, spe, FNR, y_pred_test, proba_test  = test(model_p, X_test, y_test, threshold, test=True)\n",
        "print('Test sen = ', round(sen,4),'\\n')\n",
        "print('Test spe = ', round(spe,4))\n",
        "pvalue, auc_lower_b, auc_upper_b, auc_lower_nb, auc_upper_nb, tpr_lower_b, tpr_upper_b, tpr_lower_nb, tpr_upper_nb = evaluate_disparities(model_p, proba_test, X_test, y_test, ID_test, ind_test_b, ind_test_nb)\n",
        "\n",
        "#Final ROC plotting\n",
        "proba_test_df = pd.DataFrame(data=proba_test, index=ID_test)\n",
        "y_test = pd.DataFrame(data=y_test[:,1], index = ID_test)\n",
        "\n",
        "#ROC Black\n",
        "fpr_b, tpr_b, _ = roc_curve(y_test.loc[ind_test_b], proba_test_df.loc[ind_test_b])\n",
        "roc_auc_score_b = roc_auc_score(y_test.loc[ind_test_b], proba_test_df.loc[ind_test_b])\n",
        "interp_func = interp1d(fpr_b, tpr_b)\n",
        "specified_sensitivity = interp_func(1 - specified_specificity)\n",
        "print(\"Black Sensitivity at 80% Specificity:\", specified_sensitivity)\n",
        "\n",
        "\n",
        "#ROC Non-Black\n",
        "fpr_nb, tpr_nb, _ = roc_curve(y_test.loc[ind_test_nb], proba_test_df.loc[ind_test_nb])\n",
        "roc_auc_score_nb = roc_auc_score(y_test.loc[ind_test_nb], proba_test_df.loc[ind_test_nb])\n",
        "interp_func = interp1d(fpr_nb, tpr_nb)\n",
        "specified_sensitivity = interp_func(1 - specified_specificity)\n",
        "print(\"Non-Black Sensitivity at 80% Specificity:\", specified_sensitivity)\n",
        "\n",
        "pvalue = round(pvalue, 2)\n",
        "plt.figure(figsize=[5, 5])\n",
        "plt.plot(fpr_nb, tpr_nb, color='b',\n",
        "              label=r'Non-Black AUC = %0.3f (%0.3f - %0.3f)' % (roc_auc_score_nb, auc_lower_nb, auc_upper_nb),\n",
        "              lw=2, alpha=.8)\n",
        "index_nb = (np.abs(fpr_nb - 0.2)).argmin()\n",
        "plt.scatter(fpr_nb[index_nb], tpr_nb[index_nb], lw = .2, color='black')\n",
        "plt.plot(fpr_b, tpr_b, color='#d62728',\n",
        "              label=r'Black AUC = %0.3f (%0.3f - %0.3f)' % (roc_auc_score_b, auc_lower_b, auc_upper_b),\n",
        "              lw=2, alpha=.8)\n",
        "index_b = (np.abs(fpr_b - 0.2)).argmin()\n",
        "plt.scatter(fpr_b[index_b], tpr_b[index_b], lw = .2, color='black', label=f'FPR = 0.20')\n",
        "if pvalue > 0.001:\n",
        "  plt.text(0.05, 0.95, f'p-value: {pvalue}', fontsize=11, transform=plt.gca().transAxes, color='black')\n",
        "else:\n",
        "  plt.text(0.05, 0.95, f'p << 0.001', fontsize=11, transform=plt.gca().transAxes, color='black')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', lw=1, color='grey', alpha=.5)\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False Positive Rate = 1 - Specificity')\n",
        "plt.ylabel('True Positive Rate = Sensitivity')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('Figure_AD.png', dpi=1200, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Black TPR: {tpr_lower_b}-{tpr_upper_b}\")\n",
        "print(f\"Non-Black TPR: {tpr_lower_nb}-{tpr_upper_nb}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "873lYq5CFSOu",
        "outputId": "1dfecef1-c8c7-428d-b0a3-d879eff3c45f"
      },
      "outputs": [],
      "source": [
        "from fairlearn.metrics import equalized_odds_ratio\n",
        "from fairlearn.metrics import demographic_parity_ratio\n",
        "from fairlearn.metrics import equal_opportunity_ratio\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, proba_test)\n",
        "target_fpr = 0.20\n",
        "idx = np.argmin(np.abs(fpr - 0.2))\n",
        "print(\"Index:\", idx, \"FPR:\", fpr[idx], \"Threshold:\", thresholds[idx])\n",
        "thre = thresholds[idx]\n",
        "\n",
        "\n",
        "y_pred_test = (proba_test >= thre).astype(int)\n",
        "race_test = to_categorical(race_test, num_classes=2)\n",
        "\n",
        "dp = demographic_parity_ratio(\n",
        "    y_test,\n",
        "    y_pred_test,\n",
        "    sensitive_features=race_test[:,1]\n",
        ")\n",
        "print(\"Demographic Parity Difference:\", dp)\n",
        "eod = equalized_odds_ratio(\n",
        "    y_test,\n",
        "    y_pred_test,\n",
        "    sensitive_features=race_test[:,1]\n",
        ")\n",
        "\n",
        "print(\"Equalized Odds Difference:\", eod)\n",
        "\n",
        "eop = equal_opportunity_ratio(\n",
        "    y_test,\n",
        "    y_pred_test,\n",
        "    sensitive_features=race_test[:,1]\n",
        ")\n",
        "print(\"Equal Opportunity Difference:\", eop)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
